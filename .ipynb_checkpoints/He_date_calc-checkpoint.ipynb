{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07862c6",
   "metadata": {},
   "source": [
    "# (U-Th)/He data reduction notebook for the HAL at UIUC\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook was written by William Guenthner in fall of 2022 for the reduction of grain size data, U, Th, Sm, and He measurements towards calculation of (U-Th)/He dates. Some of the inputs and file formats are specific to data generated in the Helium Analysis Laboratory (HAL) at the University of Illinois Urbana-Champaign (UIUC), but hopefully it has broader applicability and utilty for other lab groups. \n",
    "\n",
    "The notebook is structured to interact with 3 separate CSV files that should be colocated with each instance of the notebook in the same folder. The CSV files are related to: 1) U, Th, Sm, Zr, and Ca amount measurements obtained from ICP-MS anlaysis (obtained with an iCAP Q using Qtegra software at UIUC), 2) He amount measurements (obtained with a PrismaPlus 220 and reported as peak hops on masses 1-5 at UIUC), and 3) grain size measurements for Ft correction. Cells are grouped below roughly in that order of reduction (wet chemistry first, then He, then grain size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import and constants\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Avogadro = 6.022045e23 #atom/mol\n",
    "ideal_gas_moles = 22.414 #liter/mol\n",
    "\n",
    "#all in amu\n",
    "mass_233U = 233.0396280 \n",
    "mass_234U = 234.0409456\n",
    "mass_235U = 235.0439231\n",
    "mass_236U = 236.0455619\n",
    "mass_238U = 238.0507826\n",
    "mass_U = 238.028913\n",
    "\n",
    "mass_229Th = 229.031754\n",
    "mass_230Th = 230.033127\n",
    "mass_232Th = 232.0380504\n",
    "mass_Th = 232.0380504\n",
    "\n",
    "mass_144Sm = 143.911995\n",
    "mass_147Sm = 146.914893\n",
    "mass_148Sm = 147.914818\n",
    "mass_149Sm = 148.917180\n",
    "mass_150Sm = 149.917271\n",
    "mass_152Sm = 151.919728\n",
    "mass_154Sm = 153.922205\n",
    "mass_Sm = 150.366344\n",
    "\n",
    "mass_40Ca = 39.96259\n",
    "mass_42Ca = 41.95862\n",
    "mass_43Ca = 42.95877\n",
    "mass_44Ca = 43.95548\n",
    "mass_46Ca = 45.95369\n",
    "mass_48Ca = 47.95243\n",
    "mass_Ca = 40.08601\n",
    "\n",
    "mass_90Zr = 89.90470\n",
    "mass_91Zr = 90.90564\n",
    "mass_92Zr = 91.90504\n",
    "mass_94Zr = 93.90631\n",
    "mass_96Zr = 95.90828\n",
    "mass_Zr = 91.22365\n",
    "\n",
    "#all in 1/yr\n",
    "lambda_238 = 1.55125e-10\n",
    "lambda_235 = 9.84850e-10\n",
    "lambda_232 = 4.9475e-11\n",
    "lambda_147 = 6.54e-12\n",
    "\n",
    "#chemistry constants specific to HAL\n",
    "#mL\n",
    "Vnm_UTh = 0.025\n",
    "Vnm_UTh_d = Vnm_UTh * 0.01\n",
    "Vnm_Sm = 0.025\n",
    "Vnm_Sm_d = Vnm_Sm * 0.01\n",
    "#ng/mL\n",
    "concnm_U = 25.3458\n",
    "concnm_U_d = 0.0181\n",
    "concnm_Th = 49.8397\n",
    "concnm_Th_d = 0.0284\n",
    "concnm_Sm = 50.0797\n",
    "concnm_Sm_d = 0.0284"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be489e5",
   "metadata": {},
   "source": [
    "## U, Th, Sm, Zr, and Ca reduction\n",
    "\n",
    "At UIUC, we measure ratios of $^{238}$U/$^{236}$U, $^{232}$Th/$^{230}$Th, $^{152}$Sm/$^{149}$Sm, $^{90}$Zr/$^{91}$Zr, and $^{40}$Ca/$^{42}$Ca in our unknowns. For Sm, additional ratios are measured in our spike normals. The notebook is therefore designed around those specific ratios. It is also designed for the particular output format and column headers reported by an iCAP Q ICP-MS running the Qtegra software\n",
    "\n",
    "First, we open our Qtegra CSV file and extract the relevant ratios. The U_Th_file needs to be updated with the appropriate name used for the iCAP run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7210709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in name of csv file here, make sure you're in the same directory as the notebook\n",
    "U_Th_file = 'test.csv'\n",
    "\n",
    "#function to extract relevant columns from csv file\n",
    "def get_csv_data(file_name, type_string, title_string):\n",
    "    \n",
    "    with open(file_name, 'r') as in_file:\n",
    "        csv_reader_UTh = csv.reader(in_file)\n",
    "    \n",
    "        #get type of columns (Raw.Average, Raw.Ratio.STD, etc) and title (238/236 (KED), etc.)\n",
    "        run_type = next(csv_reader_UTh) \n",
    "        next(csv_reader_UTh) #throw away empty row\n",
    "        run_title = next(csv_reader_UTh)\n",
    "        next(csv_reader_UTh) #throw away this row too\n",
    "        \n",
    "        col_num = 0\n",
    "        while col_num < len(run_type) and (run_type[col_num] != type_string or run_title[col_num] != title_string):\n",
    "            col_num = col_num + 1\n",
    "        if col_num == len(run_type):\n",
    "            print('Error: column type or title not found')\n",
    "        else:\n",
    "             list_col = [0 if line[col_num]=='N/A' or line[col_num]=='' else float(line[col_num]) for line in csv_reader_UTh]\n",
    "                \n",
    "        return list_col\n",
    "\n",
    "#extract relevant columns from csv file\n",
    "list_149intensity = get_csv_data(U_Th_file, 'Raw.Average', '149Sm (KED)')\n",
    "list_152intensity = get_csv_data(U_Th_file, 'Raw.Average', '152Sm (KED)')\n",
    "list_230intensity = get_csv_data(U_Th_file, 'Raw.Average', '230Th (KED)')\n",
    "list_232intensity = get_csv_data(U_Th_file, 'Raw.Average', '232Th (KED)')\n",
    "list_236intensity = get_csv_data(U_Th_file, 'Raw.Average', '236U (KED)')\n",
    "list_236intensity = get_csv_data(U_Th_file, 'Raw.Average', '236U (KED)')\n",
    "list_238intensity = get_csv_data(U_Th_file, 'Raw.Average', '238U (KED)')\n",
    "list_238_236 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '238U (KED) / 236U (KED)')\n",
    "list_238_236_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '238U (KED) / 236U (KED)')\n",
    "list_232_230 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '232Th (KED) / 230Th (KED)')\n",
    "list_232_230_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '232Th (KED) / 230Th (KED)')\n",
    "list_152_149 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '152Sm (KED) / 149Sm (KED)')\n",
    "list_152_149_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '152Sm (KED) / 149Sm (KED)')\n",
    "list_144_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '144Sm (KED) / 152Sm (KED)')\n",
    "list_144_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '144Sm (KED) / 152Sm (KED)')\n",
    "list_147_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '147Sm (KED) / 152Sm (KED)')\n",
    "list_147_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '147Sm (KED) / 152Sm (KED)')\n",
    "list_148_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '148Sm (KED) / 152Sm (KED)')\n",
    "list_148_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '148Sm (KED) / 152Sm (KED)')\n",
    "list_150_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '150Sm (KED) / 152Sm (KED)')\n",
    "list_150_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '150Sm (KED) / 152Sm (KED)')\n",
    "list_154_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '154Sm (KED) / 152Sm (KED)')\n",
    "list_154_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '154Sm (KED) / 152Sm (KED)')\n",
    "\n",
    "\n",
    "#extract sample names from csv file\n",
    "with open(U_Th_file, 'r') as in_file:\n",
    "    col_num = 1\n",
    "    csv_reader_UTh = csv.reader(in_file)\n",
    "    for i in range (4):\n",
    "        next(csv_reader_UTh) #throw away the junk rows\n",
    "    sample_list = [str(line[col_num]) for line in csv_reader_UTh]\n",
    "\n",
    "#construct the data frame\n",
    "U_Th_dict = {'149 intensity':list_149intensity,'152 intensity':list_152intensity,'230 intensity':list_230intensity,\n",
    "             '232 intensity':list_232intensity,'236 intensity':list_236intensity,'238 intensity':list_238intensity,\n",
    "             '238/236':list_238_236,'238/236 1s':list_238_236_1s, '232/230':list_232_230, '232/230 1s':list_232_230_1s, \n",
    "             '152/149':list_152_149, '152/149 1s':list_152_149_1s,'144/152':list_144_152,'144/152 1s':list_144_152_1s,\n",
    "             '147/152':list_147_152,'147/152 1s':list_147_152_1s,'148/152':list_148_152,'148/152 1s':list_148_152_1s,\n",
    "             '150/152':list_150_152,'150/152 1s':list_150_152_1s,'154/152':list_154_152,'154/152 1s':list_154_152_1s}\n",
    "\n",
    "U_Th_data = pd.DataFrame(U_Th_dict, columns = ['149 intensity','152 intensity','230 intensity', '232 intensity', '236 intensity', \n",
    "                                      '238 intensity', '238/236','238/236 1s', '232/230', '232/230 1s', '152/149', \n",
    "                                      '152/149 1s','144/152','144/152 1s','147/152','147/152 1s','148/152',\n",
    "                                      '148/152 1s','150/152','150/152 1s','154/152','154/152 1s'], index = sample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e1e8",
   "metadata": {},
   "source": [
    "We want to check blank intensities and spike normal ratio consistency throughout the run before moving on. The next couple of cells do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report out the blanks values for 152, 232, and 238\n",
    "for i in range(0, len(sample_list)):\n",
    "    if 'AB' in sample_list[i]:\n",
    "        print('Blank levels for acid blank ',sample_list[i], ' are:\\n',list_152intensity[i], ' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')\n",
    "    elif 'BB ' in sample_list[i]:\n",
    "        print('Blank levels for bomb blank ',sample_list[i],' are:\\n',list_152intensity[i],' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')\n",
    "    elif 'Empty' in sample_list[i]:\n",
    "        print('Blank levels for Nb blank ',sample_list[i],' are:\\n',list_152intensity[i],' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b402b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot spike normal ratios throughout the run to check for consistency\n",
    "SN_U_vals = []\n",
    "SN_U_err = []\n",
    "SN_Th_vals = [] \n",
    "SN_Th_err = []\n",
    "SN_Sm_vals = []\n",
    "SN_Sm_err = []\n",
    "SN_names = []\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "#this one plots 238/236\n",
    "axs[0].xlabel('SN name')\n",
    "axs[0].ylabel('$^{238}$U/$^{236}$U')\n",
    "axs[0].title('U ratios for spike normals')\n",
    "\n",
    "#this one plots 232/230\n",
    "axs[1].xlabel('SN name')\n",
    "axs[1].ylabel('$^{232}$Th/$^{230}$Th')\n",
    "axs[1].title('Th ratios for spike normals')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(sample_list)):\n",
    "    if sample_list[i].startswith('SN'):\n",
    "        SN_names.append(i)\n",
    "        SN_U_vals.append(float(list_238_236[i]))\n",
    "        SN_U_err.append(float(list_238_236_1s[i]))\n",
    "        SN_Th_vals.append(float(list_232_230[i]))\n",
    "        SN_Th_err.append(float(list_232_230_1s[i]))\n",
    "        SN_Sm_vals.append(float(list_152_149[i]))\n",
    "        SN_Sm_err.append(float(list_152_149_1s[i]))\n",
    "axs[0].errorbar(SN_names, SN_U_vals, SN_U_err, fmt='o')\n",
    "axs[1].errorbar(SN_names, SN_Th_vals, SN_Th_err, fmt='o')\n",
    "axs[2].errorbar(SN_names, SN_Sm_vals, SN_Sm_err, fmt='o')\n",
    "\n",
    "axs[0].show()\n",
    "axs[1].show()\n",
    "axs[2].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac222d2",
   "metadata": {},
   "source": [
    "The next cell below calculates the U, Th, Sm, Zr, and Ca in our samples and saves the data in lists for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with spike calibration: calculate the mass of each spike using the spiked normals as a reference standard\n",
    "\n",
    "#function for weighted averages\n",
    "def weight_avg(ratio_list):\n",
    "    sum_W = 0\n",
    "    sum_XW = 0\n",
    "    \n",
    "    for index in ratio_list:\n",
    "        W = 1/index**2\n",
    "        XW = W * index\n",
    "        sum_W = sum_W + W\n",
    "        sum_XW = sum_XW + XW\n",
    "    \n",
    "    w_avg = sum_W/sum_XW\n",
    "    w_avg_err = 1/math.sqrt(sum_W)\n",
    "    \n",
    "    return w_avg, w_avg_err\n",
    "\n",
    "#get spike blank ratio lists\n",
    "SB_U_vals = []\n",
    "SB_U_err = []\n",
    "SB_Th_vals = [] \n",
    "SB_Th_err = []\n",
    "SB_Sm_vals = []\n",
    "SB_Sm_err = []\n",
    "\n",
    "for i in range(0, len(sample_list)):\n",
    "    if sample_list[i].startswith('SB'):\n",
    "        SB_U_vals.append(float(list_238_236[i]))\n",
    "        SB_U_err.append(float(list_238_236_1s[i]))\n",
    "        SB_Th_vals.append(float(list_232_230[i]))\n",
    "        SB_Th_err.append(float(list_232_230_1s[i]))\n",
    "    elif sample_list[i].startswith('SBSm'):        \n",
    "        SB_Sm_vals.append(float(list_152_149[i]))\n",
    "        SB_Sm_err.append(float(list_152_149_1s[i]))\n",
    "\n",
    "#calculate mass of spike in each spike normal, SW_U, SW_Th, SW_Sm\n",
    "#first up is U\n",
    "\n",
    "Rs_U = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
