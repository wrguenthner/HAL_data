{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07862c6",
   "metadata": {},
   "source": [
    "# (U-Th)/He data reduction notebook for the HAL at UIUC\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook was written by William Guenthner in fall of 2022 for the reduction of grain size data, U, Th, Sm, and He measurements towards calculation of (U-Th)/He dates. Some of the inputs and file formats are specific to data generated in the Helium Analysis Laboratory (HAL) at the University of Illinois Urbana-Champaign (UIUC), but hopefully it has broader applicability and utilty for other lab groups. \n",
    "\n",
    "The notebook is structured to interact with 3 separate CSV files that should be colocated with each instance of the notebook in the same folder. The CSV files are related to: 1) U, Th, Sm, Zr, and Ca amount measurements obtained from ICP-MS anlaysis (obtained with an iCAP Q using Qtegra software at UIUC), 2) He amount measurements (obtained with a PrismaPlus 220 and reported as peak hops on masses 1-5 at UIUC), and 3) grain size measurements for Ft correction. Cells are grouped below roughly in that order of reduction (wet chemistry first, then He, then grain size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import and constants\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Avogadro = 6.022045e23 #atom/mol\n",
    "ideal_gas_moles = 22.414 #liter/mol\n",
    "\n",
    "#all in amu\n",
    "mass_233U = 233.0396280 \n",
    "mass_234U = 234.0409456\n",
    "mass_235U = 235.0439231\n",
    "mass_236U = 236.0455619\n",
    "mass_238U = 238.0507826\n",
    "mass_U = 238.028913\n",
    "\n",
    "mass_229Th = 229.031754\n",
    "mass_230Th = 230.033127\n",
    "mass_232Th = 232.0380504\n",
    "mass_Th = 232.0380504\n",
    "\n",
    "mass_144Sm = 143.911995\n",
    "mass_147Sm = 146.914893\n",
    "mass_148Sm = 147.914818\n",
    "mass_149Sm = 148.917180\n",
    "mass_150Sm = 149.917271\n",
    "mass_152Sm = 151.919728\n",
    "mass_154Sm = 153.922205\n",
    "mass_Sm = 150.366344\n",
    "\n",
    "mass_40Ca = 39.96259\n",
    "mass_42Ca = 41.95862\n",
    "mass_43Ca = 42.95877\n",
    "mass_44Ca = 43.95548\n",
    "mass_46Ca = 45.95369\n",
    "mass_48Ca = 47.95243\n",
    "mass_Ca = 40.08601\n",
    "\n",
    "mass_90Zr = 89.90470\n",
    "mass_91Zr = 90.90564\n",
    "mass_92Zr = 91.90504\n",
    "mass_94Zr = 93.90631\n",
    "mass_96Zr = 95.90828\n",
    "mass_Zr = 91.22365\n",
    "\n",
    "#all in 1/yr\n",
    "lambda_238 = 1.55125e-10\n",
    "lambda_235 = 9.84850e-10\n",
    "lambda_232 = 4.9475e-11\n",
    "lambda_147 = 6.54e-12\n",
    "\n",
    "#chemistry constants specific to HAL\n",
    "#mL\n",
    "Vnm_UTh = 0.025\n",
    "d_Vnm_UTh = Vnm_UTh * 0.01\n",
    "Vnm_Sm = 0.025\n",
    "d_Vnm_Sm = Vnm_Sm * 0.01\n",
    "\n",
    "#abudances in normal solutions\n",
    "Ab_238U_nm = 1 - (1/137.818)\n",
    "d_Ab_238U_nm = 0\n",
    "Ab_149Sm_nm = 0.1382\n",
    "d_Ab_149Sm_nm = 0\n",
    "Ab_152Sm_nm = 0.2675\n",
    "d_Ab_152Sm_nm = 0\n",
    "\n",
    "#ng/mL\n",
    "concnm_U = 25.3458\n",
    "d_concnm_U = 0.0181\n",
    "concnm_Th = 49.8397\n",
    "d_concnm_Th = 0.0284\n",
    "concnm_Sm = 50.0797\n",
    "d_concnm_Sm = 0.0284"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be489e5",
   "metadata": {},
   "source": [
    "## U, Th, Sm, Zr, and Ca reduction\n",
    "\n",
    "At UIUC, we measure ratios of $^{238}$U/$^{236}$U, $^{232}$Th/$^{230}$Th, $^{152}$Sm/$^{149}$Sm, $^{90}$Zr/$^{91}$Zr, and $^{40}$Ca/$^{42}$Ca in our unknowns. For Sm, additional ratios are measured in our spike normals. The notebook is therefore designed around those specific ratios. It is also designed for the particular output format and column headers reported by an iCAP Q ICP-MS running the Qtegra software\n",
    "\n",
    "First, we open our Qtegra CSV file and extract the relevant ratios. The U_Th_file needs to be updated with the appropriate name used for the iCAP run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7210709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in name of csv file here, make sure you're in the same directory as the notebook\n",
    "U_Th_file = 'test.csv'\n",
    "\n",
    "#function to extract relevant columns from csv file\n",
    "def get_csv_data(file_name, type_string, title_string):\n",
    "    \n",
    "    with open(file_name, 'r') as in_file:\n",
    "        csv_reader_UTh = csv.reader(in_file)\n",
    "    \n",
    "        #get type of columns (Raw.Average, Raw.Ratio.STD, etc) and title (238/236 (KED), etc.)\n",
    "        run_type = next(csv_reader_UTh) \n",
    "        next(csv_reader_UTh) #throw away empty row\n",
    "        run_title = next(csv_reader_UTh)\n",
    "        next(csv_reader_UTh) #throw away this row too\n",
    "        \n",
    "        col_num = 0\n",
    "        while col_num < len(run_type) and (run_type[col_num] != type_string or run_title[col_num] != title_string):\n",
    "            col_num = col_num + 1\n",
    "        if col_num == len(run_type):\n",
    "            print('Error: column type or title not found')\n",
    "        else:\n",
    "             list_col = [0 if line[col_num]=='N/A' or line[col_num]=='' else float(line[col_num]) for line in csv_reader_UTh]\n",
    "                \n",
    "        return list_col\n",
    "\n",
    "#extract relevant columns from csv file\n",
    "list_149intensity = get_csv_data(U_Th_file, 'Raw.Average', '149Sm (KED)')\n",
    "list_152intensity = get_csv_data(U_Th_file, 'Raw.Average', '152Sm (KED)')\n",
    "list_230intensity = get_csv_data(U_Th_file, 'Raw.Average', '230Th (KED)')\n",
    "list_232intensity = get_csv_data(U_Th_file, 'Raw.Average', '232Th (KED)')\n",
    "list_236intensity = get_csv_data(U_Th_file, 'Raw.Average', '236U (KED)')\n",
    "list_236intensity = get_csv_data(U_Th_file, 'Raw.Average', '236U (KED)')\n",
    "list_238intensity = get_csv_data(U_Th_file, 'Raw.Average', '238U (KED)')\n",
    "list_238_236 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '238U (KED) / 236U (KED)')\n",
    "list_238_236_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '238U (KED) / 236U (KED)')\n",
    "list_232_230 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '232Th (KED) / 230Th (KED)')\n",
    "list_232_230_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '232Th (KED) / 230Th (KED)')\n",
    "list_152_149 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '152Sm (KED) / 149Sm (KED)')\n",
    "list_152_149_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '152Sm (KED) / 149Sm (KED)')\n",
    "list_144_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '144Sm (KED) / 152Sm (KED)')\n",
    "list_144_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '144Sm (KED) / 152Sm (KED)')\n",
    "list_147_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '147Sm (KED) / 152Sm (KED)')\n",
    "list_147_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '147Sm (KED) / 152Sm (KED)')\n",
    "list_148_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '148Sm (KED) / 152Sm (KED)')\n",
    "list_148_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '148Sm (KED) / 152Sm (KED)')\n",
    "list_150_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '150Sm (KED) / 152Sm (KED)')\n",
    "list_150_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '150Sm (KED) / 152Sm (KED)')\n",
    "list_154_152 = get_csv_data(U_Th_file, 'Raw.Ratio.Average', '154Sm (KED) / 152Sm (KED)')\n",
    "list_154_152_1s = get_csv_data(U_Th_file, 'Raw.Ratio.STD', '154Sm (KED) / 152Sm (KED)')\n",
    "\n",
    "\n",
    "#extract sample names from csv file\n",
    "with open(U_Th_file, 'r') as in_file:\n",
    "    col_num = 1\n",
    "    csv_reader_UTh = csv.reader(in_file)\n",
    "    for i in range (4):\n",
    "        next(csv_reader_UTh) #throw away the junk rows\n",
    "    sample_list = [str(line[col_num]) for line in csv_reader_UTh]\n",
    "\n",
    "#construct the data frame\n",
    "U_Th_dict = {'149 intensity':list_149intensity,'152 intensity':list_152intensity,'230 intensity':list_230intensity,\n",
    "             '232 intensity':list_232intensity,'236 intensity':list_236intensity,'238 intensity':list_238intensity,\n",
    "             '238/236':list_238_236,'238/236 1s':list_238_236_1s, '232/230':list_232_230, '232/230 1s':list_232_230_1s, \n",
    "             '152/149':list_152_149, '152/149 1s':list_152_149_1s,'144/152':list_144_152,'144/152 1s':list_144_152_1s,\n",
    "             '147/152':list_147_152,'147/152 1s':list_147_152_1s,'148/152':list_148_152,'148/152 1s':list_148_152_1s,\n",
    "             '150/152':list_150_152,'150/152 1s':list_150_152_1s,'154/152':list_154_152,'154/152 1s':list_154_152_1s}\n",
    "\n",
    "U_Th_data = pd.DataFrame(U_Th_dict, columns = ['149 intensity','152 intensity','230 intensity', '232 intensity', '236 intensity', \n",
    "                                      '238 intensity', '238/236','238/236 1s', '232/230', '232/230 1s', '152/149', \n",
    "                                      '152/149 1s','144/152','144/152 1s','147/152','147/152 1s','148/152',\n",
    "                                      '148/152 1s','150/152','150/152 1s','154/152','154/152 1s'], index = sample_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e1e8",
   "metadata": {},
   "source": [
    "We want to check blank intensities and spike normal ratio consistency throughout the run before moving on. The next couple of cells do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report out the blanks values for 152, 232, and 238\n",
    "for i in range(0, len(sample_list)):\n",
    "    if 'AB' in sample_list[i]:\n",
    "        print('Blank levels for acid blank ',sample_list[i], ' are:\\n',list_152intensity[i], ' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')\n",
    "    elif 'BB ' in sample_list[i]:\n",
    "        print('Blank levels for bomb blank ',sample_list[i],' are:\\n',list_152intensity[i],' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')\n",
    "    elif 'Empty' in sample_list[i]:\n",
    "        print('Blank levels for Nb blank ',sample_list[i],' are:\\n',list_152intensity[i],' for 152Sm, ',list_232intensity[i],' for 232Th, and ',list_238intensity[i],' for 238U\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b402b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot spike normal ratios throughout the run to check for consistency\n",
    "SN_U_vals = []\n",
    "SN_U_err = []\n",
    "SN_Th_vals = [] \n",
    "SN_Th_err = []\n",
    "SN_Sm_vals = []\n",
    "SN_Sm_err = []\n",
    "SN_names = []\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "#this one plots 238/236\n",
    "axs[0].set_xlabel('SN name')\n",
    "axs[0].set_ylabel('$^{238}$U/$^{236}$U')\n",
    "\n",
    "#this one plots 232/230\n",
    "axs[1].set_xlabel('SN name')\n",
    "axs[1].set_ylabel('$^{232}$Th/$^{230}$Th')\n",
    "\n",
    "#this one plots 152/149\n",
    "axs[2].set_xlabel('SN name')\n",
    "axs[2].set_ylabel('$^{152}$Sm/$^{149}$Sm')\n",
    "\n",
    "for i in range(0, len(sample_list)):\n",
    "    if sample_list[i].startswith('SN'):\n",
    "        SN_names.append(i)\n",
    "        SN_U_vals.append(float(list_238_236[i]))\n",
    "        SN_U_err.append(float(list_238_236_1s[i]))\n",
    "        SN_Th_vals.append(float(list_232_230[i]))\n",
    "        SN_Th_err.append(float(list_232_230_1s[i]))\n",
    "        SN_Sm_vals.append(float(list_152_149[i]))\n",
    "        SN_Sm_err.append(float(list_152_149_1s[i]))\n",
    "axs[0].errorbar(SN_names, SN_U_vals, SN_U_err, fmt='o')\n",
    "axs[1].errorbar(SN_names, SN_Th_vals, SN_Th_err, fmt='o')\n",
    "axs[2].errorbar(SN_names, SN_Sm_vals, SN_Sm_err, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac222d2",
   "metadata": {},
   "source": [
    "The next cell below calculates the spike mass of U, Th, Sm, Zr, and Ca delivered to each of our samples using the spiked normal solutions as a reference. These values are assigned to the variables Sw_U, Sw_Th, Sw_Sm, Sw_Ca, and Sw_Zr that will be used in the next set of cells below this one. The fundamental equation for each element looks like this:\n",
    "\n",
    "### $N_w = S_w * \\frac{W_N}{W_S} * \\frac{Ab_S^A - R_m*Ab_S^B}{R_m*Ab_N^B - Ab_N^A}$\n",
    "\n",
    "which is rearranged to solve for $S_w$ such that:\n",
    "\n",
    "### $S_w = N_w * \\frac{W_S}{W_N} * \\frac{R_m*Ab_N^B - Ab_N^A}{Ab_S^A - R_m*Ab_S^B}$\n",
    "\n",
    "where: \n",
    "$N_w$ is the weight of the element in the normal solution,\n",
    "$S_w$ is the weight of the element in the spike solution,\n",
    "$W_N$ is the atomic weight of the element in the normal solution,\n",
    "$W_S$ is the atomic weight of the element in the spike solution,\n",
    "$Ab_S^A$ is the abundance of isotope A in the spike,\n",
    "$Ab_S^B$ is the abuandance of isotope B in the spike,\n",
    "$Ab_N^A$ is the abundance of isotope A in the normal solution,\n",
    "$Ab_N^B$ is the abundance of isotope B in the spike solution, and\n",
    "$R_m$ is the measured ratio of the reference isotope over the enriched isotope in the spike. Each of these variables needs to have partial deriviatives calculated for error propagation, and this is also performed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with spike calibration: calculate the mass of each spike using the spiked normals as a reference standard\n",
    "\n",
    "#function for weighted averages\n",
    "def weight_avg(ratio_list):\n",
    "    sum_W = 0\n",
    "    sum_XW = 0\n",
    "    \n",
    "    for index in ratio_list:\n",
    "        W = 1/index**2\n",
    "        XW = W * index\n",
    "        sum_W = sum_W + W\n",
    "        sum_XW = sum_XW + XW\n",
    "    \n",
    "    w_avg = sum_W/sum_XW\n",
    "    w_avg_err = 1/math.sqrt(sum_W)\n",
    "    \n",
    "    return w_avg, w_avg_err\n",
    "\n",
    "#function for calculating S_w\n",
    "def spike_mass(Ab_A_nm, d_Ab_A_nm, Ab_B_nm, d_Ab_B_nm, Ab_A_spk, d_Ab_A_spk, Ab_B_spk, d_Ab_B_spk, Rm_spknm, d_Rm_spknm, Nw, d_Nw, Ws, d_Ws, Wn, d_Wn):\n",
    "    \n",
    "    #use some terms here for ease of doing error propagation: omega, gamma, kappa, zeta\n",
    "    omega = Nw*Ws*1/Wn\n",
    "    gamma = (Rm_spknm*Ab_B_nm - Ab_A_nm)/(Ab_A_spk - Rm_spknm*Ab_B_spk)\n",
    "    kappa = Ab_A_spk - Rm_spknm*Ab_B_spk\n",
    "    zeta = omega*Rm_spknm*Ab_B_nm - omega*Ab_A_nm\n",
    "    \n",
    "    #calculate all the partial derivatives for error propagation\n",
    "    d_Sw_d_Nw = Ws * gamma/Wn\n",
    "    d_SW_d_Ws = Nw * gamma/Wn\n",
    "    d_Sw_d_Wn = -omega * gamma/Wn\n",
    "    d_Sw_d_Rm = (kappa*omega*Ab_B_nm + zeta*Ab_B_spk)/kappa**2\n",
    "    d_Sw_d_Ab_A_nm = -omega/kappa\n",
    "    d_Sw_d_Ab_B_nm = omega*Rm_spknm/kappa\n",
    "    d_Sw_d_Ab_A_spk = -zeta/kappa**2\n",
    "    d_Sw_d_Ab_B_spk = Rm_spknm*zeta/kappa\n",
    "    \n",
    "    #calculate Sw and d_Sw using terms above\n",
    "    Sw = omega * gamma\n",
    "    d_Sw = math.sqrt(d_Nw**2*d_Sw_d_Nw**2 + d_Ws**2*d_Sw_d_Ws**2 + d_Wn**2*d_Sw_d_Wn**2 + \n",
    "                     d_Rm_spknm**2*d_Sw_d_Rm**2 + d_Ab_B_nm**2*d_Sw_d_Ab_B_nm**2 + d_Ab_A_nm**2*d_Sw_d_Ab_A_nm**2 + \n",
    "                     d_Ab_B_spk**2*d_Sw_d_Ab_B_spk**2 + d_Ab_A_spk**2*d_Sw_d_Ab_A_spk**2)\n",
    "    \n",
    "    return Sw, d_Sw\n",
    "\n",
    "#get spike blank ratio lists\n",
    "SB_U_vals = []\n",
    "SB_Th_vals = [] \n",
    "SB_Sm152_149_vals = []\n",
    "SB_Sm154_152_vals = []\n",
    "SB_Sm150_152_vals = []\n",
    "SB_Sm148_152_vals = []\n",
    "SB_Sm147_152_vals = []\n",
    "SB_Sm144_152_vals = []\n",
    "\n",
    "for i in range(0, len(sample_list)):\n",
    "    if sample_list[i].startswith('SB'):\n",
    "        SB_U_vals.append(float(list_238_236[i]))\n",
    "        SB_Th_vals.append(float(list_232_230[i]))\n",
    "    elif sample_list[i].startswith('SBSm'):        \n",
    "        SB_Sm152_149_vals.append(float(list_152_149[i]))\n",
    "        SB_Sm154_152_vals.append(float(list_154_152[i]))\n",
    "        SB_Sm150_152_vals.append(float(list_150_152[i]))\n",
    "        SB_Sm148_152_vals.append(float(list_148_152[i]))\n",
    "        SB_Sm147_152_vals.append(float(list_147_152[i]))\n",
    "        SB_Sm144_152_vals.append(float(list_144_152[i]))\n",
    "\n",
    "#calculate mass of spike in each spike normal, SW_U, SW_Th, SW_Sm\n",
    "\n",
    "#first up is U\n",
    "#natural/spike ratio in spike blank used to get abundances of isotopes in the spike\n",
    "Rs_U, d_Rs_U = weight_avg(SB_U_vals)\n",
    "Ab_235U_spk = 1/(1+Rs_U)\n",
    "d_Ab_235U_spk = math.sqrt(d_Rs_U**2 * (-1/(1+Rs_U)**2)**2)\n",
    "Ab_236U_spk = 1/(1+Rs_U/137.818+Rs_U)\n",
    "d_Ab_236U_spk = math.sqrt(d_Rs_U**2 * (-1*(1/137.818+1)/(1+Rs_U*(1/137.818+1))^2)**2)\n",
    "Ab_238U_spk = (1-Ab_236_spk)/(1+1/137.818)\n",
    "d_Ab_238U_spk = math.sqrt(d_Ab_236_spk**2 * (-1*1/(1+1/137.818))**2)\n",
    "\n",
    "#atomic weight of U in spike (g/mol)\n",
    "Ws_U = Ab_235U_spk*mass_235U + Ab_236U_spk*mass_236U + Ab_238U_spk*mass_238U\n",
    "d_Ws_U = math.sqrt(d_Ab_235U_spk**2 * mass_235U**2 + d_Ab_236U_spk**2 * mass_236U**2 + d_Ab_238U_spk**2 * mass_238U**2)\n",
    "\n",
    "#weight of U in the normal solution (ng)\n",
    "Nw_U = Vnm_UTh*concm_U\n",
    "d_Nw_U = math.sqrt(d_Vnm_UTh**2*concm_U**2 + d_concm_U**2*Vnm_UTh**2)\n",
    "\n",
    "#get weighted average of spike normals\n",
    "Rm_spknm_U, d_Rm_spknm_U = weight_avg(SN_U_vals)\n",
    "\n",
    "#mass of U spike in each spike normal (ng), used in subsequent calculations for mass of U in unknowns \n",
    "Sw_U, d_Sw_U = spike_mass(Ab_238U_nm, d_Ab_238U_nm, 0, 0, Ab_238U_spk, d_Ab_238U_spk, Ab_236U_spk, d_Ab_236U_spk, \n",
    "                          Rm_spknm_U, d_Rm_spknm_U, Nw_U, d_Nw_U, Ws_U, d_Ws_U, mass_U, 0)\n",
    "\n",
    "\n",
    "#next up is Th\n",
    "#natural/spike ratio in spike blank used to get abundances of isotopes in the spike\n",
    "Rs_Th, d_Rs_Th = weight_avg(SB_Th_vals)\n",
    "Ab_230Th_spk = 1/(1+Rs_Th)\n",
    "d_Ab_230Th_spk = math.sqrt(d_Rs_Th**2 * (-1/(1+Rs_Th)**2)**2)\n",
    "Ab_232Th_spk = 1 - Ab_230Th_spk\n",
    "d_Ab_232Th_spk = math.sqrt(d_Ab_230Th_spk**2 * 1**2)\n",
    "\n",
    "#atomic weight of Th in spike (g/mol)\n",
    "Ws_Th = Ab_230Th_spk*mass_230Th + Ab_232Th_spk*mass_232Th\n",
    "d_Ws_Th = math.sqrt(d_Ab_230Th_spk**2 * mass_230Th**2 + d_Ab_232Th_spk**2 * mass_232Th**2)\n",
    "\n",
    "#weight of Th in the normal solution (ng)\n",
    "Nw_Th = Vnm_UTh*concm_Th\n",
    "d_Nw_Th = math.sqrt(d_Vnm_UTh**2*concm_Th**2 + d_concm_Th**2*Vnm_UTh**2)\n",
    "\n",
    "#get weighted average of spike normals\n",
    "Rm_spknm_Th, d_Rm_spknm_Th = weight_avg(SN_Th_vals)\n",
    "\n",
    "#mass of Th spike in each spike normal (ng), used in subsequent calculations for mass of U in unknowns \n",
    "Sw_Th, d_Sw_Th = spike_mass(Ab_232Th_nm, d_Ab_232Th_nm, 0, 0, Ab_232Th_spk, d_Ab_232Th_spk, Ab_230Th_spk, d_Ab_230Th_spk, \n",
    "                          Rm_spknm_Th, d_Rm_spknm_Th, Nw_Th, d_Nw_Th, Ws_Th, d_Ws_Th, mass_Th, 0)\n",
    "\n",
    "\n",
    "#now we do Sm\n",
    "#natural/spike ratio in spike blank used to get abundances of isotopes in the spike\n",
    "Rs_Sm, d_Rs_Sm = weight_avg(SB_Sm152_149_vals)\n",
    "Rs_154_152, d_Rs_154_152 = weight_avg(SB_Sm154_152_vals)\n",
    "Rs_150_152, d_Rs_150_152 = weight_avg(SB_Sm150_152_vals)\n",
    "Rs_148_152, d_Rs_148_152 = weight_avg(SB_Sm148_152_vals)\n",
    "Rs_147_152, d_Rs_147_152 = weight_avg(SB_Sm147_152_vals)\n",
    "Rs_144_152, d_Rs_144_152 = weight_avg(SB_Sm144_152_vals)\n",
    "delta_Sm = 1 + Rs_154_152 + Rs_150_152 + Rs_148_152 + Rs_147_152 + Rs_144_152\n",
    "\n",
    "Ab_149Sm_spk = 1/(1 + Rs_Sm*delta_Sm)\n",
    "d_Ab_149Sm_spk = math.sqrt(d_Rs_Sm**2*(-delta_Sm/(1+Rs_Sm*delta_Sm)**2)**2 + \n",
    "                           d_Rs_144_152**2*(-Rs_Sm/(1+Rs_Sm*delta_Sm)**2)**2 + \n",
    "                           d_Rs_147_152**2*(-Rs_Sm/(1+Rs_Sm*delta_Sm)**2)**2 + \n",
    "                           d_Rs_148_152**2*(-Rs_Sm/(1+Rs_Sm*delta_Sm)**2)**2 + \n",
    "                           d_Rs_150_152**2*(-Rs_Sm/(1+Rs_Sm*delta_Sm)**2)**2 + \n",
    "                           d_Rs_154_152**2*(-Rs_Sm/(1+Rs_Sm*delta_Sm)**2)**2)\n",
    "Ab_152Sm_spk = Rs_Sm * Ab_149Sm_spk\n",
    "d_Ab_152Sm_spk = math.sqrt(d_Rs_Sm**2*Ab_149Sm_spk**2 + d_Ab_149Sm_spk**2*Rs_Sm**2)\n",
    "\n",
    "Ab_144Sm_spk = Ab_152Sm_spk*Rs_144_152\n",
    "d_Ab_144Sm_spk = math.sqrt(d_Rs_144_152**2*Ab_152Sm_spk**2 + d_Ab_152Sm_spk**2*Rs_144_152**2)\n",
    "Ab_147Sm_spk = Ab_152Sm_spk*Rs_147_152\n",
    "d_Ab_147Sm_spk = math.sqrt(d_Rs_147_152**2*Ab_152Sm_spk**2 + d_Ab_152Sm_spk**2*Rs_147_152**2)\n",
    "Ab_148Sm_spk = Ab_152Sm_spk*Rs_148_152\n",
    "d_Ab_148Sm_spk = math.sqrt(d_Rs_148_152**2*Ab_152Sm_spk**2 + d_Ab_152Sm_spk**2*Rs_148_152**2)\n",
    "Ab_150Sm_spk = Ab_152Sm_spk*Rs_150_152\n",
    "d_Ab_150Sm_spk = math.sqrt(d_Rs_150_152**2*Ab_152Sm_spk**2 + d_Ab_152Sm_spk**2*Rs_150_152**2)\n",
    "Ab_154Sm_spk = Ab_152Sm_spk*Rs_154_152\n",
    "d_Ab_154Sm_spk = math.sqrt(d_Rs_154_152**2*Ab_152Sm_spk**2 + d_Ab_152Sm_spk**2*Rs_154_152**2)\n",
    "\n",
    "#atomic weight of Sm in spike (g/mol)\n",
    "Ws_Sm = Ab_144Sm_spk*mass_144Sm + Ab_147Sm_spk*mass_147Sm + Ab_148Sm_spk*mass_148Sm + Ab_149Sm_spk*mass_149Sm +\n",
    "            Ab_150Sm_spk*mass_150Sm + Ab_152Sm_spk*mass_152Sm + Ab_154Sm_spk*mass_154Sm\n",
    "d_Ws_Sm = math.sqrt(d_Ab_144Sm_spk**2*mass_144Sm**2 + d_Ab_147Sm_spk**2*mass_147Sm**2 + d_Ab_148Sm_spk**2*mass_148Sm**2 \n",
    "                    + d_Ab_149Sm_spk**2*mass_149Sm**2 + d_Ab_150Sm_spk**2*mass_150Sm**2 + d_Ab_152Sm_spk**2*mass_152Sm**2 \n",
    "                    + d_Ab_154Sm_spk**2*mass_154Sm**2)\n",
    "\n",
    "#weight of Sm in the normal solution (ng)\n",
    "Nw_Sm = Vnm_Sm*concm_Sm\n",
    "d_Nw_Sm = math.sqrt(d_Vnm_Sm**2*concm_Sm**2 + d_concm_Sm**2*Vnm_Sm**2)\n",
    "\n",
    "#get weighted average of spike normals\n",
    "Rm_spknm_Sm, d_Rm_spknm_Sm = weight_avg(SN_Sm_vals)\n",
    "\n",
    "#mass of Sm spike in each spike normal (ng), used in subsequent calculations for mass of U in unknowns \n",
    "Sw_Sm, d_Sw_Sm = spike_mass(Ab_152Sm_nm, d_Ab_152Sm_nm, Ab_149Sm_nm, d_Ab_149Sm_nm, Ab_152Sm_spk, d_Ab_152Sm_spk, Ab_149Sm_spk, d_Ab_149Sm_spk, \n",
    "                          Rm_spknm_Sm, d_Rm_spknm_Sm, Nw_Sm, d_Nw_Sm, Ws_Sm, d_Ws_Sm, mass_Sm, 0)\n",
    "\n",
    "print('Mass of spike values are: ',Sw_U,'+/-',d_Sw_U,' for U, ',Sw_Th,'+/-',d_Sw_Th,' for Th, and ',Sw_Sm,'+/-',d_Sw_Sm,' for Sm.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
