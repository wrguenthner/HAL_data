{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He_line_calc: a notebook for reducing He line data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook reduces data produced by a Pfieffer PrismaPlus220 in the Helium Analysis Laboratory (HAL) at the University of Illinois. The HAL uses h6t and Pychron software for data reporting and measures masses 1-5 on line blanks, hot blanks, line gas standards, and samples. We use an isotope dilution approach with a $^3He$ spike and a $^4He$ reference gas of a known volume. As such, 4/3 gas ratios are measured with corrections for H, D, and HD. This notebook reads in the raw data files from either h6t or Pychron and reports out He amounts in terms of pmol.\n",
    "\n",
    "Instructions for the use of this notebook are provided before each cell of code and should be followed step-by-step. Only one notebook is needed for a complete set of analyses that may encompass multiple days (referred throughout in this notebook as a \"session\"). Some cells will be run at the very beginning of data collection, whereas other cells will be run repeatedly as new data is collected. __Pay attention as to which cells need to be run once, and which will be repeatedly run throughout the data collection process!__ If you are unfamiliar with Jupyter Notebooks, the \"run\" button is found at the top of the notebook, and each cell can be run by highlighting it with the mouse and then clicking the \"run\" button.\n",
    "\n",
    "### Step-by-step instructions\n",
    "\n",
    "First, check to see if there is any output left over for any of the cells from the last time the notebook was run. If there is, then go to Kernel -> Restart & Clear Output before proceeding.\n",
    "\n",
    "This first cell imports some useful packages and sets some constants used throughout the notebook. __This cell only needs to be run once.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll {height: 44em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>div.output_scroll {height: 44em;}</style>\"))\n",
    "pd.options.display.max_rows = 700\n",
    "\n",
    "#constants\n",
    "ideal_gas_moles = 22.414 #liter/mol\n",
    "initial_tank_4He = 6.8630\n",
    "initial_tank_4He_1s = 0.0489\n",
    "tank_depletion = 0.999981\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will find the project folder that you're working in. In most cases, it will only need to be run once; however, there may be occasions (e.g. error with file path or change to file path) that you may want to run it multiple times. If you do need to switch file directories for whatever reason, make sure the cell below is run __before__ any subsequent cells. \n",
    "\n",
    "A possible exception to this regards whether or not you want to upload a saved summary csv file. In that case, and depending on the circumstances, you may want to run the \"load csv cell\" (3 cells below) in the notebook before re-running the cell immediately below. Note that if you do that, __a new csv file might be created and saved to a different folder.__ Finally, __be sure to fill in the project name variable within the ''!__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'RS24_GZ8_diff_cell'\n",
    "\n",
    "#set file path, uncomment based on directory structure applicable to your work\n",
    "#the first option is for the current directory, others are more verbose paths specific to HAL computers\n",
    "\n",
    "file_path = os.getcwd()\n",
    "#file_path = os.path.join('C:\\\\','Users','lab-admin','Documents','GitHub','HAL_data','projects_2023',project)\n",
    "#file_path = os.path.join('/Users','wrg','Documents','GitHub','HAL_data','projects_2023',project)\n",
    "#file_path = os.path.join('/Users','wrg','Documents','GitHub','HAL_data','sandbox')\n",
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell below allows you to reload your list of aliquots in case you have to exit the notebook before a project is complete. You can skip this cell if you're starting a new project without a pre-existing csv file. If you need to load a csv file to continue to work on a project do so only after running the first cell of the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an aliquot_frame from a saved csv file to add more data\n",
    "#this obviously requires that such a file already exists \n",
    "\n",
    "aliquot_frame = pd.read_csv(project + '_He_data.csv',index_col=0)\n",
    "\n",
    "display(aliquot_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell of code reads each individual run file produced by h6t. You will need to __run this cell everytime you collect new data from the PrismaPlus__. In the cell below, enter:\n",
    "\n",
    "1. the name of the aliquot in the aliquot variable \n",
    "2. the QST number in the QST variable (changes every aliquot) \n",
    "3. the QRT number in the QRT variable (changes only for standards)\n",
    "4. the type of analysis (see comments in code cell for strings to use)\n",
    "\n",
    "__For string variables, you will need to type within the ''.__ The file extension is added to the aliquot name in the next line of code. The aliquot name should be the name of the sample, line blank, hot blank, or line standard. Line blanks have the style: 'lbXX_mmddyyyy' where XX = the number for the line blank on that day (01, 02, 03 ... 10, 11, etc.), mm = the month, dd = the day, and yyyy = the year. Hot blanks have the style: 'hbXX_mmddyyyy'. Line standards have the style: 'stdXXXX' where XXXX is the shot number from the $^4$He pipette as recorded in the notebook and listed in the 4HeTank actuation number (the shot number or QRT is double the actuation number). Re-extracts are appended to the end of the sample name with the convention: 'sample_reX' where 'X' is the re-extract number (1-4). __Follow these naming conventions!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "coroutine raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f97e466c0337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#throw away first line of headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_reader_He\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: coroutine raised StopIteration"
     ]
    }
   ],
   "source": [
    "#fill in information here, edited for diffusion experiment\n",
    "#for the aliquot type use:\n",
    "#'lb' for line blank, 'hb' for hot blank, 'std' for standard, 'zirc' for zircon, 'ap' for apatite\n",
    "\n",
    "aliquot = 'lb02_08122024'\n",
    "aliquot_type = 'lb'\n",
    "QST = 10484\n",
    "QRT = 1669\n",
    "\n",
    "\n",
    "#finding the file\n",
    "file_name_list = os.listdir('.')\n",
    "actuation = str(QST*2)\n",
    "\n",
    "for file in file_name_list:\n",
    "    if actuation in file:\n",
    "        file_name = file\n",
    "        \n",
    "#creating prisma data list\n",
    "Prisma_data_list = []\n",
    "with open(file_name, mode='r') as in_file:\n",
    "    csv_reader_He = csv.reader(in_file)\n",
    "    \n",
    "    #throw away first line of headers\n",
    "    next(csv_reader_He)\n",
    "    \n",
    "    #read in the data lines to a list of lists until the end\n",
    "    for line in csv_reader_He:\n",
    "        #throw away empty rows in between lines\n",
    "        if any(line):\n",
    "            Prisma_data = [float(i) for i in line[:7]]\n",
    "            Prisma_data_list.append(Prisma_data)\n",
    "\n",
    "\n",
    "#convert data_list to an array for easier indexing\n",
    "Prisma_data_array=np.array(Prisma_data_list)\n",
    "\n",
    "aliquot_time = Prisma_data_array[-1,1]\n",
    "\n",
    "#create time list (x-values) and corrected 4He/3He list (y-values)\n",
    "t_list = [(Prisma_data_array[i,1]-Prisma_data_array[0,1]) for i in range(len(Prisma_data_array))]\n",
    "He_ratio_list = [(Prisma_data_array[i,5]-Prisma_data_array[i,6])/(Prisma_data_array[i,4]-Prisma_data_array[i,6]-0.005*Prisma_data_array[i,2]) for i in range(len(Prisma_data_array))]\n",
    "\n",
    "#do some math to find the intercept and mean of the corrected 4He/3He\n",
    "sum_t_y = 0\n",
    "sum_t2 = 0\n",
    "sum_slope_err = 0\n",
    "\n",
    "for i in range(len(t_list)):\n",
    "    sum_t_y = sum_t_y + t_list[i]*He_ratio_list[i]\n",
    "    sum_t2 = sum_t2 + t_list[i]**2\n",
    "\n",
    "slope = (len(t_list)*sum_t_y - sum(t_list)*sum(He_ratio_list))/(len(t_list)*sum_t2 - sum(t_list)**2)\n",
    "intercept = (sum(He_ratio_list) - slope*sum(t_list))/len(t_list)\n",
    "\n",
    "for i in range(len(t_list)):\n",
    "    sum_slope_err = sum_slope_err + (He_ratio_list[i] - intercept - slope*t_list[i])**2\n",
    "\n",
    "del_slope = math.sqrt(sum_slope_err/(len(t_list) - 2)) * math.sqrt(len(t_list)/(len(t_list)*sum_t2 - sum(t_list)**2))\n",
    "del_intercept = math.sqrt(sum_slope_err/(len(t_list) - 2)) * math.sqrt(sum_t2/(len(t_list)*sum_t2 - sum(t_list)**2))\n",
    "\n",
    "mean_4He_3He = np.mean(He_ratio_list)\n",
    "stdev_4He_3He = np.std(He_ratio_list)\n",
    "\n",
    "print('The intercept and error for {0}, listed as a {1}, is: '.format(aliquot,aliquot_type))\n",
    "print('{0:.5f} +/- {1:.5f}'.format(intercept,del_intercept))\n",
    "print('and the mean and std dev is: ')\n",
    "print('{0:.5f} +/- {1:.5f}'.format(mean_4He_3He,stdev_4He_3He))\n",
    "\n",
    "#precent of re-extract relative to original sample\n",
    "if aliquot[-4:-1] == '_re':\n",
    "    \n",
    "    aliquot_extractions = 0\n",
    "    aliquot_total = intercept\n",
    "    hb_time_diff = aliquot_time\n",
    "    aliquot_prefix = aliquot.split('_re')\n",
    "    \n",
    "    for i in range(len(aliquot_frame)):\n",
    "        temp_name = aliquot_frame.index[i]\n",
    "        \n",
    "        #find all of the re-extracts for a sample\n",
    "        if temp_name.startswith(aliquot_prefix[0]):\n",
    "            aliquot_total = aliquot_total + aliquot_frame.intercept[i]\n",
    "            aliquot_extractions = aliquot_extractions + 1\n",
    "        \n",
    "        #find the nearest (in time) hot blank\n",
    "        if aliquot_frame.aliquot_type[i] == 'hb' and abs(aliquot_frame.time_of_collection[i] - aliquot_time) < hb_time_diff:\n",
    "            closest_hb = aliquot_frame.intercept[i]\n",
    "            hb_time_diff = abs(aliquot_frame.time_of_collection[i] - aliquot_time)\n",
    "    \n",
    "    reextract_percent = 100 * (intercept - closest_hb)/(aliquot_total - aliquot_extractions * closest_hb)\n",
    "    \n",
    "    print('The percent of this re-extract from the total gas release so far is: {0:.2f}'.format(reextract_percent))\n",
    "\n",
    "#show graph of masses through time\n",
    "seconds_list = []\n",
    "for i in range(len(Prisma_data_array[:,1])):\n",
    "    time_since_epoch = datetime.fromtimestamp(Prisma_data_array[i,1])\n",
    "    seconds_list.append(mdate.date2num(time_since_epoch))\n",
    "date_fmt = '%m-%d-%y %H:%M:%S'\n",
    "date_formatter = mdate.DateFormatter(date_fmt)\n",
    "\n",
    "plt.figure(figsize = (14,9))\n",
    "plt.ylabel('intensity')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('time')\n",
    "plt.plot(seconds_list, Prisma_data_array[:,2], label = 'H')\n",
    "plt.plot(seconds_list, Prisma_data_array[:,3], label = 'H2 + D')\n",
    "plt.plot(seconds_list, Prisma_data_array[:,4], label = 'HD + 3He')\n",
    "plt.plot(seconds_list, Prisma_data_array[:,5], label = '4He')\n",
    "plt.plot(seconds_list, Prisma_data_array[:,6], label = 'background')\n",
    "plt.gca().xaxis.set_major_formatter(date_formatter)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to add these data to the running line summary list for your run. __If you do not want the data to be added to the running list do not run the next cell.__ There are various reasons why you might not want to add this particular aliquot to the running total. For example, typically the first line blank of the day comes in with a slightly high 4He/3He ratio due to static conditions in the line (valves closed for a prolonged period of time) and we discard it.\n",
    "\n",
    "If you do want to add these data to the line summary list, click run in the next cell below to proceed. __Add any important sample notes to the sample data frame by filling in the '' next to the notes variable.__ The assumption here is that all data added to a given line summary list will be in chronologic order. This becomes particularly important for re-extract calculation. __Make sure that the sample extraction tied to a specific set of re-extracts is loaded into the aliquot frame before the re-extract.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sample/run to sample data frame\n",
    "#FOR DIFFUSION EXPERIMENT, add extraction temp and time\n",
    "#enter in any notes you have in the notes variable below\n",
    "\n",
    "# temp in degree C and time in seconds, 'None' for lb,hb,std\n",
    "\n",
    "diff_temp = None\n",
    "diff_time = None\n",
    "cool_time = None\n",
    "\n",
    "notes = ''\n",
    "\n",
    "#creates a new aliquot_frame if none exists, edited for diffusion experiment\n",
    "if 'aliquot_frame' not in locals():\n",
    "    aliquot_frame = pd.DataFrame(columns = ['aliquot_type','intercept','error','QST','QRT','diff_temp',\n",
    "                                            'diff_time','cool_time','notes','time_of_collection','file'], index = [])\n",
    "\n",
    "file_check = False\n",
    "for name_of_file in aliquot_frame.file:\n",
    "    if name_of_file == file_name:\n",
    "        file_check = True\n",
    "        \n",
    "if file_check:\n",
    "    print(\"You've used the same file twice! Double check your QST and rerun this cell!\")\n",
    "else:\n",
    "    row_data = pd.DataFrame({'aliquot_type':aliquot_type.lower(), 'intercept':intercept,'error':del_intercept,\n",
    "                             'QST':QST, 'QRT':QRT, 'diff_temp':diff_temp, 'diff_time':diff_time, 'cool_time':cool_time, 'notes':notes,\n",
    "                             'time_of_collection':aliquot_time,'file':file_name },\n",
    "                            columns = ['aliquot_type','intercept','error','QST','QRT','diff_temp','diff_time','cool_time','notes','time_of_collection','file'], index = [aliquot])\n",
    "    aliquot_frame = pd.concat([aliquot_frame, row_data])\n",
    "    #updates the file everytime you add a row, comment out the line below to stop overwriting the file\n",
    "    aliquot_frame.to_csv(project + '_He_data.csv')  \n",
    "\n",
    "display(aliquot_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last step organizes and reduces your data for export to a json file that we use to further reduce the data once U, Th, and Sm information has been collected. Here, the notebook reports out blank corrected, 4He volumes (in ncc) and amounts (in nmols) for each sample aliquot. We use the hot blank as the blank correction as it most closely resembles the valve procedures for running samples. The line blank could be substituted here. __You will only need to run this once at the end to export the data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize reference list of lists and sample dictionary, ref lists tie running means to specific samples\n",
    "std_num_list = []\n",
    "hb_ref_list = []\n",
    "lb_ref_list = []\n",
    "std_ref_list = []\n",
    "sample_dict = {}\n",
    "\n",
    "#initialize sample times and reference numbers used to tie samples to relevant running means \n",
    "last_hb_time = 0\n",
    "last_lb_time = 0\n",
    "last_std_time = 0\n",
    "std_ref_num = 0\n",
    "lb_ref_num = 0\n",
    "hb_ref_num = 0\n",
    "\n",
    "#selection statements to determine type of aliquot and what to do with it\n",
    "#blanks and standards have additional selection statements in order to reset running means at the start of a new analysis day\n",
    "#re-extracts are dealt with by adding their totals to the correct sample\n",
    "#re-extracts require that the bulk sample extraction comes BEFORE the re-extract in the aliquot frame\n",
    "#edited sample_dict[aliquot_name] for diffusion experiment\n",
    "\n",
    "for i in range(len(aliquot_frame)):\n",
    "    \n",
    "    aliquot_name = aliquot_frame.index[i]\n",
    "    aliquot_time = aliquot_frame.time_of_collection[i]\n",
    "    aliquot_date = date.fromtimestamp(aliquot_time)\n",
    "    aliquot_hour = datetime.fromtimestamp(aliquot_time).hour\n",
    "    \n",
    "    if aliquot_frame.aliquot_type[i] == 'hb':\n",
    "        if aliquot_date == date.fromtimestamp(last_hb_time) or aliquot_hour < 5:\n",
    "            hb_list.append(aliquot_frame.intercept[i])\n",
    "            hb_mean = np.mean(hb_list)\n",
    "            hb_err = np.std(hb_list)\n",
    "            hb_ref_list[hb_ref_num] = [hb_mean, hb_err]\n",
    "            last_hb_time = aliquot_time\n",
    "        else:\n",
    "            #reset hb_list if it's a new analysis \"day\", 5 am used here as cutoff as some users might work after midnight\n",
    "            hb_list = []\n",
    "            hb_list.append(aliquot_frame.intercept[i])\n",
    "            hb_ref_list.append([hb_list[0], aliquot_frame.error[i]])\n",
    "            last_hb_time = aliquot_time\n",
    "            if len(hb_ref_list)>1:\n",
    "                hb_ref_num = hb_ref_num + 1\n",
    "    elif aliquot_frame.aliquot_type[i] == 'lb':\n",
    "        if aliquot_date == date.fromtimestamp(last_lb_time) or aliquot_hour < 5:\n",
    "            lb_list.append(aliquot_frame.intercept[i])\n",
    "            lb_mean = np.mean(lb_list)\n",
    "            lb_err = np.std(lb_list)\n",
    "            lb_ref_list[lb_ref_num] = [lb_mean, lb_err]\n",
    "            last_lb_time = aliquot_time\n",
    "        else:\n",
    "            #reset lb_list if it's a new analysis \"day\", 5 am used here as cutoff as some users might work after midnight\n",
    "            lb_list = []\n",
    "            lb_list.append(aliquot_frame.intercept[i])\n",
    "            lb_ref_list.append([lb_list[0], aliquot_frame.error[i]])\n",
    "            last_lb_time = aliquot_time\n",
    "            if len(lb_ref_list)>1:\n",
    "                lb_ref_num = lb_ref_num + 1\n",
    "    elif aliquot_frame.aliquot_type[i] == 'std':\n",
    "        if aliquot_date == date.fromtimestamp(last_std_time) or aliquot_hour < 5:\n",
    "            std_list.append(aliquot_frame.intercept[i])\n",
    "            std_num_list.append(aliquot_frame.QRT[i])\n",
    "            std_num = np.mean(std_num_list)\n",
    "            std_mean = np.mean(std_list)\n",
    "            std_err = np.std(std_list)\n",
    "            std_ref_list[std_ref_num] = [std_mean, std_err, std_num]\n",
    "            last_std_time = aliquot_time\n",
    "        else:\n",
    "            #reset std_list if it's a new analysis \"day\", 5 am used here as cutoff as some users might work after midnight\n",
    "            std_list = []\n",
    "            std_list.append(aliquot_frame.intercept[i])\n",
    "            std_num_list = []\n",
    "            std_num_list.append(aliquot_frame.QRT[i])\n",
    "            std_ref_list.append([std_list[0], aliquot_frame.error[i], std_num_list[0]])\n",
    "            last_std_time = aliquot_time\n",
    "            if len(std_ref_list)>1:\n",
    "                std_ref_num = std_ref_num + 1\n",
    "    elif aliquot_name[-4:-1] == '_re':\n",
    "        reextract_sample = aliquot_name.split('_re')\n",
    "        update_list = sample_dict[reextract_sample[0]]\n",
    "        update_list[0] = update_list[0] + aliquot_frame.intercept[i]\n",
    "        update_list[1] = update_list[1] + aliquot_frame.error[i]\n",
    "        update_list[2] = update_list[2] + 1\n",
    "        sample_dict[reextract_sample[0]] = update_list\n",
    "    else:      \n",
    "        sample_dict[aliquot_name] = [aliquot_frame.intercept[i], aliquot_frame.error[i], 0, aliquot_time, \n",
    "                                     lb_ref_num, hb_ref_num, std_ref_num, aliquot_frame.notes[i], aliquot_frame.aliquot_type[i], \n",
    "                                     aliquot_frame.diff_temp[i], aliquot_frame.diff_time[i], aliquot_frame.cool_time[i]]\n",
    "\n",
    "#csv file for Ft entry that gets reported out\n",
    "csv_out_file = project + '_Ft_data.csv'\n",
    "\n",
    "with open(csv_out_file, 'w', newline='') as Ft_file:\n",
    "    csv_writer = csv.writer(Ft_file)\n",
    "    csv_writer.writerow(['L1, L2 = maximum length of Crystal (including tips, in two orientations)'])\n",
    "    csv_writer.writerow(['W1, W2 = mutually perpendicular widths of orthorhombic prism or equatorial diameters'])\n",
    "    csv_writer.writerow(['h1, h2 = tip heights of pyramidal terminations (zircons only)'])\n",
    "    csv_writer.writerow(['AI = abrasion index:'])\n",
    "    csv_writer.writerow(['apatite: 1 = both tips present, 1.5 = one tip missing, 2 = both tips missing, DUR = durango'])\n",
    "    csv_writer.writerow(['zircon: 1-5 (1=euhedral, 5=rounded, 3=still has pyramids)'])\n",
    "    csv_writer.writerow('')\n",
    "    \n",
    "    header = ['sample_name','morph_comments', 'AI', 'L1_(um)', 'L2_(um)', 'W1_(um)', 'W2_(um)', 'h1_(um)', 'h2_(um)']\n",
    "    sample_names = [name for name in sample_dict]\n",
    "    csv_writer.writerow(header)\n",
    "    for line in sample_names:\n",
    "        csv_writer.writerow([line])\n",
    "     \n",
    "#json dictionary that gets reported out\n",
    "json_out_dict = {}\n",
    "\n",
    "#for loop that steps through elements of sample_dict, converts to volume and amount, and adds to the json_out_dict\n",
    "for index in sample_dict:\n",
    "    \n",
    "    std_ncc = initial_tank_4He * tank_depletion**std_ref_list[sample_dict[index][6]][2]\n",
    "    std_ncc_1s = initial_tank_4He_1s * tank_depletion**std_ref_list[sample_dict[index][6]][2]\n",
    "    sample_vol = ((sample_dict[index][0] - (sample_dict[index][2] + 1)*hb_ref_list[sample_dict[index][5]][0])/\n",
    "                    (std_ref_list[sample_dict[index][6]][0]-hb_ref_list[sample_dict[index][5]][0])) * std_ncc\n",
    "    sample_mol = sample_vol * 1e-12/ideal_gas_moles\n",
    "    \n",
    "    #propagate errors\n",
    "    d_ncc4He_d_samp = std_ncc/(sample_dict[index][0] - hb_ref_list[sample_dict[index][5]][0])\n",
    "    d_ncc4He_d_stdncc = (sample_dict[index][0] - hb_ref_list[sample_dict[index][5]][0])/std_ncc\n",
    "    d_ncc4He_d_std = -(sample_dict[index][0] - hb_ref_list[sample_dict[index][5]][0])*std_ncc/(std_ref_list[sample_dict[index][6]][0] - hb_ref_list[sample_dict[index][5]][0])**2\n",
    "    sample_vol_err = math.sqrt(sample_dict[index][1]**2 * d_ncc4He_d_samp**2 + std_ncc_1s**2 * d_ncc4He_d_stdncc**2\n",
    "                               + std_ref_list[sample_dict[index][6]][1]**2 * d_ncc4He_d_std**2)\n",
    "    sample_mol_err = sample_vol_err * 1e-12/ideal_gas_moles\n",
    "    \n",
    "    #add to the json_dict, edited for diffusion exp\n",
    "    json_out_dict[index] = [sample_vol, sample_vol_err, sample_mol, sample_mol_err, sample_dict[index][7], sample_dict[index][9], sample_dict[index][10], sample_dict[index][11]]\n",
    "    \n",
    "#send the dict to a json file\n",
    "json_file = project + '_He_data.json'\n",
    "\n",
    "with open(json_file, 'w') as out_file:\n",
    "    json.dump(json_out_dict, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have successfully exported your data and confirmed that the file is safe and sound, save and exit the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c7602c82e39efa19a01e5e068584db7a6d17aff8711ab06660aac81377393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
